Steps to install and run the project on your localhost.

1. Fork the repository.
2. run the command 'npm i ' in one terminal.  
3. then run the command 'python app.py' in one terminal. 
4. open a seperate terminal and run the command 'npx tailwindcss -i ./static/main.css -o ./static/output.css --watch'. 

Changes required 

1. design and make seperate cards for every object of result
   run the code 
    import pandas as pd
    import torch
    from transformers import T5Tokenizer, T5ForConditionalGeneration
    from torch.utils.data import DataLoader, Dataset
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    from transformers import AdamW
    from tqdm import tqdm
    
    # Load the dataset
    df = pd.read_csv("Book1.csv")
    
    # Check unique labels in the dataset
    unique_labels = df['label'].unique()
    num_labels = len(unique_labels)
    print("Number of unique labels:", num_labels)
    print("Unique labels:", unique_labels)
    
    # Verify label range
    expected_labels = set(range(num_labels))
    if set(unique_labels) != expected_labels:
        print("Error: Labels are not within the expected range.")
        # You may need to fix the labels in your dataset.
    
    # Verify label encoding
    if df['label'].isnull().sum() > 0:
        print("Error: There are missing values in the label column.")
        # You may need to handle missing values in your dataset.
    
    # Split dataset into training and validation sets
    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)
    
    # Define dataset class
    class CustomDataset(Dataset):
        def __init__(self, dataframe, tokenizer, max_length=128):
            self.tokenizer = tokenizer
            self.data = dataframe
            self.max_length = max_length
    
        def __len__(self):
            return len(self.data)
    
        def __getitem__(self, idx):
            text = str(self.data.iloc[idx]['text'])
            label = self.data.iloc[idx]['label']
    
            inputs = self.tokenizer.encode_plus(
                text,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
    
            return {
                'input_ids': inputs['input_ids'].flatten(),
                'attention_mask': inputs['attention_mask'].flatten(),
                'labels': torch.tensor(label, dtype=torch.long)
            }
    
    # Initialize the tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained('t5-large')
    model = T5ForConditionalGeneration.from_pretrained('t5-large')
    
    # Define training parameters
    batch_size = 4
    epochs = 3
    learning_rate = 2e-5
    
    # Create DataLoader for training and validation sets
    train_dataset = CustomDataset(train_df, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    val_dataset = CustomDataset(val_df, tokenizer)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # Define optimizer and loss function
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    criterion = torch.nn.CrossEntropyLoss()
    
    # Training loop
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batches'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
    
            optimizer.zero_grad()
    
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()
    
            loss.backward()
            optimizer.step()
    
        average_train_loss = total_loss / len(train_loader)
        print(f'Training Loss: {average_train_loss}')
    
        # Validation loop
        model.eval()
        val_predictions = []
        val_targets = []
    
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f'Validation', unit='batches'):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
    
                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=1)
                preds = torch.argmax(outputs, dim=2).squeeze()
    
                val_predictions.extend(preds.cpu().numpy())
                val_targets.extend(labels.cpu().numpy())
    
        val_accuracy = accuracy_score(val_targets, val_predictions)
        print(f'Validation Accuracy: {val_accuracy}')
    
    # Define the directory path where you want to save the model
    save_directory = "./saved_model_t5"
    
    # Create the directory if it doesn't exist
    import os
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)
    
    # Save the model and tokenizer
    model.save_pretrained(save_directory)
    tokenizer.save_pretrained(save_directory)
    
    print("Model saved successfully!") 
    
2.  make the card in the given format of sample card(the doc file     provided).
3. iterate through every object of the "result" and return every object into seperate cards. the "type" should be bold and larger than the regular text, it will be used as the header of the card.
4. the "content" object of the result will be used as rhe description of the card. 

5. fill colours and make the ui simplistic. 